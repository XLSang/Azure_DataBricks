{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c447b34-330c-42f1-89e0-0edc0b17f970"}}},{"cell_type":"markdown","source":["# Bronze to Silver - ETL into a Silver table\n\nWe need to perform some transformations on the data to move it from bronze to silver tables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73e618a3-d715-45b0-a750-f37542f932d5"}}},{"cell_type":"markdown","source":["## Notebook Objective\n\nIn this notebook we:\n1. Ingest raw data using composable functions\n1. Use composable functions to write to the Bronze table\n1. Develop the Bronze to Silver Step\n   - Extract and transform the raw string to columns\n   - Quarantine the bad data\n   - Load clean data into the Silver table\n1. Update the status of records in the Bronze table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f87a061-db90-4f9a-a25b-4a6e6679044b"}}},{"cell_type":"markdown","source":["## Step Configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d80dceca-ce4c-4424-87ac-cbfa34aa32ed"}}},{"cell_type":"code","source":["%run ./includes/configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11a9626c-2e5a-4ed4-9933-8fb9edecc824"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Import Operation Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d1ee8af-a728-46f9-86a5-8f21568410fe"}}},{"cell_type":"code","source":["%run ./includes/main/python/operations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d12350b-ecf3-41f5-808f-2015c533663c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Display the Files in the Bronze Paths"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80992b62-ce6c-4adf-b567-9c0b6d25dc2a"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(bronzePath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fcd7ec66-fead-49b6-84a6-63dd347732ce"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Land More Raw Data\n\nBefore we get started with this lab, let's land some more raw data.\n\nIn a production setting, we might have data coming in every\nhour. Here we are simulating this with the function\n`ingest_classic_data`.\n\n😎 Recall that we did this in the notebook `00_ingest_raw`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"003a608e-0909-4df7-bd1a-5cd209161705"}}},{"cell_type":"markdown","source":["**EXERCISE:** Land ten hours using the utility function, `ingest_classic_data`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1fd61e9-cc77-4196-8bb3-31ce5838eadc"}}},{"cell_type":"code","source":["# ANSWER\ningest_classic_data(hours=10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d66b2bf8-f758-4bef-8264-9f90ab1fd5c3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Current Delta Architecture\nNext, we demonstrate everything we have built up to this point in our\nDelta Architecture.\n\nWe do so not with the ad hoc queries as written before, but now with\ncomposable functions included in the file `classic/includes/main/python/operations`.\nYou should check this file for the correct arguments to use in the next\nthree steps.\n\n🤔 You can refer to `plus/02_bronze_to_silver` if you are stuck."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c17136a2-c6c2-49be-a0f8-2be147fb3eb0"}}},{"cell_type":"markdown","source":["### Step 1: Create the `rawDF` DataFrame\n\n**Exercise:** Use the function `read_batch_raw` to ingest the newly arrived\ndata."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f70c5737-9444-4e22-a513-273c00d7f0d7"}}},{"cell_type":"code","source":["# ANSWER\nrawDF = read_batch_raw(rawPath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2576e8af-2722-4f83-aea8-cf6e70f5d3b4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 2: Transform the Raw Data\n\n**Exercise:** Use the function `transform_raw` to ingest the newly arrived\ndata."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b21ea196-d502-421d-92a3-977899825e5a"}}},{"cell_type":"code","source":["# ANSWER\ntransformedRawDF = transform_raw(rawDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15dd4b20-d649-4044-a9c8-a51d22336030"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Verify the Schema with an Assertion\n\nThe DataFrame `transformedRawDF` should now have the following schema:\n\n```\ndatasource: string\ningesttime: timestamp\nstatus: string\nvalue: string\np_ingestdate: date\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"122b5488-67da-48f8-b021-d7205e25bc93"}}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nassert transformedRawDF.schema == StructType(\n    [\n        StructField(\"datasource\", StringType(), False),\n        StructField(\"ingesttime\", TimestampType(), False),\n        StructField(\"status\", StringType(), False),\n        StructField(\"value\", StringType(), True),\n        StructField(\"p_ingestdate\", DateType(), False),\n    ]\n)\nprint(\"Assertion passed.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e32bd682-874c-4671-ba65-e81434c9792f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 3: Write Batch to a Bronze Table\n\n**Exercise:** Use the function `batch_writer` to ingest the newly arrived\ndata.\n\n**Note**: you will need to begin the write with the `.save()` method on\nyour writer.\n\n🤖 **Be sure to partition on `p_ingestdate`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7ffd2a1-5d5a-4990-813a-7fecb336b3bc"}}},{"cell_type":"code","source":["# ANSWER\nrawToBronzeWriter = batch_writer(\n    dataframe=transformedRawDF, partition_column=\"p_ingestdate\"\n)\n\nrawToBronzeWriter.save(bronzePath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54c19e66-dff2-4a67-aaee-bd43cde93741"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Purge Raw File Path\n\nManually purge the raw files that have already been loaded."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8906bcee-915c-4449-aea5-48e2f8be5a80"}}},{"cell_type":"code","source":["dbutils.fs.rm(rawPath, recurse=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05c1b690-5ae6-43e5-98a1-88545478cddb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Display the Bronze Table\n\nIf you have ingested 16 hours you should see 160 records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f34424e-4efc-40a5-9a21-238d758d6e0e"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM health_tracker_classic_bronze"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ecb07520-aef0-4255-90af-9e814bed1264"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Bronze to Silver Step\n\nLet's start the Bronze to Silver step."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7d07dd9-939a-4a19-a16a-6028e688b7c4"}}},{"cell_type":"markdown","source":["## Make Notebook Idempotent"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"784b5da7-3e42-4ba0-bc7d-13bbb9954da6"}}},{"cell_type":"code","source":["dbutils.fs.rm(silverPath, recurse=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57c6351e-ed24-4e2b-a803-9f89038bec3a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Load New Records from the Bronze Records\n\n**EXERCISE**\n\nLoad all records from the Bronze table with a status of `\"new\"`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a2e6368-2edc-41b1-b2c9-c194bea1f91f"}}},{"cell_type":"code","source":["# ANSWER\n\nbronzeDF = spark.read.table(\"health_tracker_classic_bronze\").filter(\"status = 'new'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce96882c-635b-48b3-b886-21e74ed5e740"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Extract the Nested JSON from the Bronze Records"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"128cdfea-7c80-432e-972c-f72b5df38e6e"}}},{"cell_type":"markdown","source":["### Step 1: Extract the Nested JSON from the `value` column\n**EXERCISE**\n\nUse `pyspark.sql` functions to extract the `\"value\"` column as a new\ncolumn `\"nested_json\"`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0209c63f-3785-4d5d-aeda-bb56041ae123"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import from_json\n\njson_schema = \"\"\"\n    time TIMESTAMP,\n    name STRING,\n    device_id STRING,\n    steps INTEGER,\n    day INTEGER,\n    month INTEGER,\n    hour INTEGER\n\"\"\"\n\nbronzeAugmentedDF = bronzeDF.withColumn(\n    \"nested_json\", from_json(col(\"value\"), json_schema)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8bf55b2-adbc-425a-ba58-6c15e20216f8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 2: Create the Silver DataFrame by Unpacking the `nested_json` Column\n\nUnpacking a JSON column means to flatten the JSON and include each top level attribute\nas its own column.\n\n🚨 **IMPORTANT** Be sure to include the `\"value\"` column in the Silver DataFrame\nbecause we will later use it as a unique reference to each record in the\nBronze table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b40279f-53a4-46a2-8780-1c3d3f47a309"}}},{"cell_type":"code","source":["# ANSWER\nsilver_health_tracker = bronzeAugmentedDF.select(\"value\", \"nested_json.*\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94ca0003-fa59-4959-8034-172236cb3745"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Verify the Schema with an Assertion\n\nThe DataFrame `silver_health_tracker` should now have the following schema:\n\n```\nvalue: string\ntime: timestamp\nname: string\ndevice_id: string\nsteps: integer\nday: integer\nmonth: integer\nhour: integer\n```\n\n💪🏼 Remember, the function `_parse_datatype_string` converts a DDL format schema string into a Spark schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18d1f360-e38d-4d3d-96c5-f1e1d0e08b74"}}},{"cell_type":"code","source":["from pyspark.sql.types import _parse_datatype_string\n\nassert silver_health_tracker.schema == _parse_datatype_string(\n    \"\"\"\n  value STRING,\n  time TIMESTAMP,\n  name STRING,\n  device_id STRING,\n  steps INTEGER,\n  day INTEGER,\n  month INTEGER,\n  hour INTEGER\n\"\"\"\n)\nprint(\"Assertion passed.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24681a8d-11e0-4535-a0a5-daec6513db6b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Transform the Data\n\n1. Create a column `p_eventdate DATE` from the column `time`.\n1. Rename the column `time` to `eventtime`.\n1. Cast the `device_id` as an integer.\n1. Include only the following columns in this order:\n   1. `value`\n   1. `device_id`\n   1. `steps`\n   1. `eventtime`\n   1. `name`\n   1. `p_eventdate`\n\n💪🏼 Remember that we name the new column `p_eventdate` to indicate\nthat we are partitioning on this column.\n\n🕵🏽‍♀️ Remember that we are keeping the `value` as a unique reference to values\nin the Bronze table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1172db28-980a-4fa1-8c9b-6387980a128e"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import col\n\nsilver_health_tracker = silver_health_tracker.select(\n    \"value\",\n    col(\"device_id\").cast(\"integer\").alias(\"device_id\"),\n    \"steps\",\n    col(\"time\").alias(\"eventtime\"),\n    \"name\",\n    col(\"time\").cast(\"date\").alias(\"p_eventdate\"),\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d450bb45-dbad-4de9-8187-52d6b343a958"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Verify the Schema with an Assertion\n\nThe DataFrame `silver_health_tracker_data_df` should now have the following schema:\n\n```\nvalue: string\ndevice_id: integer\nheartrate: double\neventtime: timestamp\nname: string\np_eventdate: date```\n\n💪🏼 Remember, the function `_parse_datatype_string` converts a DDL format schema string into a Spark schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa3bf4c2-b4ed-43cd-b178-7a3e8dd36d68"}}},{"cell_type":"code","source":["from pyspark.sql.types import _parse_datatype_string\n\nassert silver_health_tracker.schema == _parse_datatype_string(\n    \"\"\"\n  value STRING,\n  device_id INTEGER,\n  steps INTEGER,\n  eventtime TIMESTAMP,\n  name STRING,\n  p_eventdate DATE\n\"\"\"\n), \"Schemas do not match\"\nprint(\"Assertion passed.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"251b3584-5d75-494a-abc6-f70ef5d9c943"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Quarantine the Bad Data\n\nRecall that at step, `00_ingest_raw`, we identified that some records were coming in\nwith device_ids passed as uuid strings instead of string-encoded integers.\nOur Silver table stores device_ids as integers so clearly there is an issue\nwith the incoming data.\n\nIn order to properly handle this data quality issue, we will quarantine\nthe bad records for later processing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"727228f8-981a-4c05-bbcd-33f996a3b970"}}},{"cell_type":"markdown","source":["Check for records that have nulls - compare the output of the following two cells"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ed61c2e-4bce-427b-92b8-95543d691609"}}},{"cell_type":"code","source":["silver_health_tracker.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fabf16e-7770-4590-bab2-d3d62bd18a98"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["silver_health_tracker.na.drop().count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c2d9bd6-f788-4524-bc96-852ba2d93ec4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Split the Silver DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8a843f6-bcf5-4cf9-80f8-4f06a00c6e35"}}},{"cell_type":"code","source":["silver_health_tracker_clean = silver_health_tracker.filter(\"device_id IS NOT NULL\")\nsilver_health_tracker_quarantine = silver_health_tracker.filter(\"device_id IS NULL\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd31c93f-08e7-4da5-8071-93c844cb4963"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Display the Quarantined Records"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf306e07-f1ec-406a-bf49-7da6b02249c9"}}},{"cell_type":"code","source":["display(silver_health_tracker_quarantine)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac415941-13c4-47a9-b10b-916c483966eb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## WRITE Clean Batch to a Silver Table\n\n**EXERCISE:** Batch write `silver_health_tracker_clean` to the Silver table path, `silverPath`.\n\n1. Use format, `\"delta\"`\n1. Use mode `\"append\"`.\n1. Do **NOT** include the `value` column.\n1. Partition by `\"p_eventdate\"`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c4a82b2-a343-480f-8fb9-6daf3aae436c"}}},{"cell_type":"code","source":["# ANSWER\n(\n    silver_health_tracker_clean.select(\n        \"device_id\", \"steps\", \"eventtime\", \"name\", \"p_eventdate\"\n    )\n    .write.format(\"delta\")\n    .mode(\"append\")\n    .partitionBy(\"p_eventdate\")\n    .save(silverPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ba7f0ce-c08b-47d9-8e20-14eb0949eb57"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\n    \"\"\"\nDROP TABLE IF EXISTS health_tracker_classic_silver\n\"\"\"\n)\n\nspark.sql(\n    f\"\"\"\nCREATE TABLE health_tracker_classic_silver\nUSING DELTA\nLOCATION \"{silverPath}\"\n\"\"\"\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70c71c5c-66d6-4d86-9651-0198a2c07dca"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Verify the Schema with an Assertion"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0819fc29-3312-4f73-aba6-25d6bc8e6191"}}},{"cell_type":"code","source":["silverTable = spark.read.table(\"health_tracker_classic_silver\")\nexpected_schema = \"\"\"\n  device_id INTEGER,\n  steps INTEGER,\n  eventtime TIMESTAMP,\n  name STRING,\n  p_eventdate DATE\n\"\"\"\n\nassert silverTable.schema == _parse_datatype_string(\n    expected_schema\n), \"Schemas do not match\"\nprint(\"Assertion passed.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"555733bf-e57a-4339-bfc6-d76350f0c1bb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n\nSELECT * FROM health_tracker_classic_silver"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39b310a8-10cb-4155-b9ae-db314a8f4e2d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Update Bronze table to Reflect the Loads\n\n**EXERCISE:** Update the records in the Bronze table to reflect updates.\n\n### Step 1: Update Clean records\nClean records that have been loaded into the Silver table and should have\n   their Bronze table `status` updated to `\"loaded\"`.\n\n💃🏽 **Hint** You are matching the `value` column in your clean Silver DataFrame\nto the `value` column in the Bronze table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"720b5fb1-c283-48e7-b059-5bdca02e4ae9"}}},{"cell_type":"code","source":["# ANSWER\nfrom delta.tables import DeltaTable\n\nbronzeTable = DeltaTable.forPath(spark, bronzePath)\nsilverAugmented = silver_health_tracker_clean.withColumn(\"status\", lit(\"loaded\"))\n\nupdate_match = \"bronze.value = clean.value\"\nupdate = {\"status\": \"clean.status\"}\n\n(\n    bronzeTable.alias(\"bronze\")\n    .merge(silverAugmented.alias(\"clean\"), update_match)\n    .whenMatchedUpdate(set=update)\n    .execute()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11252d30-cb99-4200-86b7-d4721e4d4336"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**EXERCISE:** Update the records in the Bronze table to reflect updates.\n\n### Step 2: Update Quarantined records\nQuarantined records should have their Bronze table `status` updated to `\"quarantined\"`.\n\n🕺🏻 **Hint** You are matching the `value` column in your quarantine Silver\nDataFrame to the `value` column in the Bronze table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92baab68-f14a-4565-b922-17492db0820f"}}},{"cell_type":"code","source":["# ANSWER\nsilverAugmented = silver_health_tracker_quarantine.withColumn(\n    \"status\", lit(\"quarantined\")\n)\n\nupdate_match = \"bronze.value = quarantine.value\"\nupdate = {\"status\": \"quarantine.status\"}\n\n(\n    bronzeTable.alias(\"bronze\")\n    .merge(silverAugmented.alias(\"quarantine\"), update_match)\n    .whenMatchedUpdate(set=update)\n    .execute()\n)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c084220-02b6-4910-8e23-53935c92c4e2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d869c08-aef7-4efb-8490-f5da45f5f2d0"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"02_bronze_to_silver","dashboards":[],"notebookMetadata":null,"language":"python","widgets":{},"notebookOrigID":1990275853788908}},"nbformat":4,"nbformat_minor":0}
