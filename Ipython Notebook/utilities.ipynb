{"cells":[{"cell_type":"code","source":["\nfrom pyspark.sql.session import SparkSession\nfrom urllib.request import urlretrieve\nfrom pyspark.sql.functions import from_unixtime, dayofmonth, month, hour\nfrom delta import DeltaTable\nfrom datetime import datetime\nimport time\n\nCLASSIC_DATA = \"classic_data_2020_h1.snappy.parquet\"\nCLASSIC_DELTA = \"classic_data_2020_h1.delta\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29f6e164-549a-438a-9049-82efb6eb5c2d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def retrieve_data(file: str, landingPath: str) -> bool:\n    \"\"\"Download file from remote location to driver. Move from driver to DBFS.\"\"\"\n\n    base_url = \"https://files.training.databricks.com/static/data/health-tracker/\"\n    url = base_url + file\n    driverPath = \"file:/databricks/driver/\" + file\n    dbfsPath = landingPath + file\n    urlretrieve(url, file)\n    dbutils.fs.mv(driverPath, dbfsPath)\n    return True\n\n\ndef prepare_activity_data(landingPath) -> bool:\n    retrieve_data(CLASSIC_DATA, landingPath)\n\n    classicIngest = (\n        spark.read.format(\"parquet\")\n        .load(landingPath + CLASSIC_DATA)\n        .withColumn(\"time\", from_unixtime(\"time\"))\n        .select(\n            \"*\",\n            dayofmonth(\"time\").alias(\"day\"),\n            month(\"time\").alias(\"month\"),\n            hour(\"time\").alias(\"hour\"),\n        )\n        .write.format(\"delta\")\n        .save(landingPath + CLASSIC_DELTA)\n    )\n\n\ndef ingest_classic_data(hours: int = 1) -> bool:\n    CLASSIC_DELTA = \"classic_data_2020_h1.delta\"\n    classicDelta = spark.read.format(\"delta\").load(landingPath + CLASSIC_DELTA)\n\n    next_batch = classicDelta.orderBy(\"month\", \"day\", \"hour\").limit(10 * hours)\n    file_name = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n\n    (next_batch.write.format(\"json\").save(rawPath + file_name))\n\n    # move file out of directory and rename\n    new_json_file = [\n        file.path for file in dbutils.fs.ls(rawPath + file_name) if \"part\" in file.path\n    ][0]\n    dbutils.fs.mv(new_json_file, rawPath + file_name + \".txt\")\n    dbutils.fs.rm(rawPath + file_name, recurse=True)\n\n    classicIngest = DeltaTable.forPath(spark, landingPath + CLASSIC_DELTA)\n\n    delete_match = \"\"\"\n        ingest.name = next.name AND\n        ingest.time = next.time\n    \"\"\"\n\n    (\n        classicIngest.alias(\"ingest\")\n        .merge(next_batch.alias(\"next\"), delete_match)\n        .whenMatchedDelete()\n        .execute()\n    )\n\n    return True\n\n\ndef untilStreamIsReady(namedStream: str, progressions: int = 3) -> bool:\n    queries = list(filter(lambda query: query.name == namedStream, spark.streams.active))\n    while len(queries) == 0 or len(queries[0].recentProgress) < progressions:\n        time.sleep(5)\n        queries = list(filter(lambda query: query.name == namedStream, spark.streams.active))\n    print(\"The stream {} is active and ready.\".format(namedStream))\n    return True\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f871d80c-9015-48b7-84c9-e232dac3e0e6"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"utilities","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1990275853789071}},"nbformat":4,"nbformat_minor":0}
