{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83b487e7-d6dc-4afd-a5bb-f68f99cba1d4"}}},{"cell_type":"markdown","source":["# Raw to Bronze Pattern"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"691b304c-a9cc-45bc-b0ab-982e810edd29"}}},{"cell_type":"markdown","source":["## Notebook Objective\n\nIn this notebook we:\n1. Ingest Raw Data\n2. Augment the data with Ingestion Metadata\n3. Batch write the augmented data to a Bronze Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"417a2de1-bc77-4a23-800d-cc215ac491f7"}}},{"cell_type":"markdown","source":["## Step Configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fbd7ddd-f2f9-49e4-a33f-fe4b8a8ca27b"}}},{"cell_type":"code","source":["%run ./includes/configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e140d23-4279-49c4-888e-4399a1fd170c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Display the Files in the Raw Path"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fc606fc-5804-4d78-9c86-44fba5c8af27"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(rawPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be650060-e954-48b5-8f25-65f3bdd41239"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Make Notebook Idempotent"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31f20914-7d1f-41a4-a025-3b6609f29afa"}}},{"cell_type":"code","source":["dbutils.fs.rm(bronzePath, recurse=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef04b6dd-1717-48e4-bcdd-23475edd1681"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Ingest raw data\n\nNext, we will read files from the source directory and write each line as a string to the Bronze table.\n\nðŸ¤  You should do this as a batch load using `spark.read`\n\nRead in using the format, `\"text\"`, and using the provided schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2ab485d-856b-4d5e-a96f-dab4532c1888"}}},{"cell_type":"code","source":["# ANSWER\nkafka_schema = \"value STRING\"\n\nraw_health_tracker_data_df = (\n    spark.read.format(\"text\").schema(kafka_schema).load(rawPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27b5927b-67fc-4324-bfe8-3a1e7e025cb3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Display the Raw Data\n\nðŸ¤“ Each row here is a raw string in JSON format, as would be passed by a stream server like Kafka."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"050e44c2-cbb0-480b-adc4-a499375340f2"}}},{"cell_type":"code","source":["display(raw_health_tracker_data_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b368b3cc-c935-46c6-ac3f-f8ebaa53867a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Ingestion Metadata\n\nAs part of the ingestion process, we record metadata for the ingestion.\n\n**EXERCISE:** Add metadata to the incoming raw data. You should add the following columns:\n\n- data source (`datasource`), use `\"files.training.databricks.com\"`\n- ingestion time (`ingesttime`)\n- status (`status`), use `\"new\"`\n- ingestion date (`ingestdate`)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46003c50-24d1-42c0-aa3b-e853c993b0ab"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import current_timestamp, lit\n\nraw_health_tracker_data_df = raw_health_tracker_data_df.select(\n    \"value\",\n    lit(\"files.training.databricks.com\").alias(\"datasource\"),\n    current_timestamp().alias(\"ingesttime\"),\n    lit(\"new\").alias(\"status\"),\n    current_timestamp().cast(\"date\").alias(\"ingestdate\"),\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"070cba59-bdbd-486d-847c-f43fb4adc368"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## WRITE Batch to a Bronze Table\n\nFinally, we write to the Bronze Table.\n\nMake sure to write in the correct order (`\"datasource\"`, `\"ingesttime\"`, `\"value\"`, `\"status\"`, `\"p_ingestdate\"`).\n\nMake sure to use following options:\n\n- the format `\"delta\"`\n- using the append mode\n- partition by `p_ingestdate`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5bcbdb9-6c7d-4a10-9c27-76bf0f57caba"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import col\n\n(\n    raw_health_tracker_data_df.select(\n        \"datasource\",\n        \"ingesttime\",\n        \"value\",\n        \"status\",\n        col(\"ingestdate\").alias(\"p_ingestdate\"),\n    )\n    .write.format(\"delta\")\n    .mode(\"append\")\n    .partitionBy(\"p_ingestdate\")\n    .save(bronzePath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df7c65fb-0a8a-4d6e-92ea-c0fc0679aa97"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(dbutils.fs.ls(bronzePath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b90f31f3-4af3-43e6-ae94-0c153562c755"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Register the Bronze Table in the Metastore\n\nThe table should be named `health_tracker_classic_bronze`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32e7e355-c659-4c4e-a6c8-620aaf157615"}}},{"cell_type":"code","source":["# ANSWER\nspark.sql(\n    \"\"\"\nDROP TABLE IF EXISTS health_tracker_classic_bronze\n\"\"\"\n)\n\nspark.sql(\n    f\"\"\"\nCREATE TABLE health_tracker_classic_bronze\nUSING DELTA\nLOCATION \"{bronzePath}\"\n\"\"\"\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5252a46c-f264-42b2-be84-ac52431b877d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Display Classic Bronze Table\n\nRun this query to display the contents of the Classic Bronze Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93d7cdcc-05f0-443e-9792-ef0570e5f4a7"}}},{"cell_type":"code","source":["%sql\n\nSELECT * FROM health_tracker_classic_bronze"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61337a1f-492c-481d-bf88-71ce379ccfba"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Query Broken Records\n\n\nRun a SQL query to display just the incoming records for \"Gonzalo ValdÃ©s\".\n\nðŸ§  You can use the SQL operator `RLIKE`, which is short for regex `LIKE`,\nto create your matching predicate.\n\n[`RLIKE` documentation](https://docs.databricks.com/spark/latest/spark-sql/language-manual/functions.html#rlike)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5275640a-6098-45c4-9584-b80b8db03955"}}},{"cell_type":"code","source":["%sql\n\nSELECT * FROM health_tracker_classic_bronze WHERE value RLIKE 'Gonzalo ValdÃ©s'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b7cd71c-7fd6-4d43-837f-3d95abe64edc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### What do you notice?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eead2cf9-540e-4cd6-8225-4a56bbe67543"}}},{"cell_type":"markdown","source":["### Display the User Dimension Table\n\n\nRun a SQL query to display the records in `health_tracker_user`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9be4603d-8086-4f77-bdb2-86c32486c72e"}}},{"cell_type":"code","source":["%sql\n\nSELECT * FROM health_tracker_user"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8de6fb41-597b-4cbe-83c6-0630002cd58d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Purge Raw File Path\n\nWe have loaded the raw files using batch loading, whereas with the Plus pipeline we used Streaming.\n\nThe impact of this is that batch does not use checkpointing and therefore does not know which files have been ingested.\n\nWe need to manually purge the raw files that have been loaded."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c112530f-10ac-4a9f-a3e4-a7ab5787df99"}}},{"cell_type":"code","source":["dbutils.fs.rm(rawPath, recurse=True)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41bba35f-7d19-499b-ab82-135868cfe64e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13849ba8-ffd3-4177-8d15-ea37027ee844"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"01_raw_to_bronze","dashboards":[],"notebookMetadata":null,"language":"python","widgets":{},"notebookOrigID":1990275853788969}},"nbformat":4,"nbformat_minor":0}
